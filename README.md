[![Docker](https://img.shields.io/badge/docker-ready-blue)](https://www.docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![GitHub Stars](https://img.shields.io/github/stars/LakshyaSingh354/Inferyx?style=social)](https://github.com/LakshyaSingh354/Inferyx)


# Inferyx - A Production-Grade AI Inference Engine âš™ï¸ğŸ”¥

**Inferyx** is a no-BS simulation of a real-world AI inference system â€” built from the ground up to mirror the insane complexity and pressure of production-scale ML deployment.  
From batching and retry queues to observability and dynamic worker pools, Inferyx doesnâ€™t play around.

It mimics what happens *after* a `/predict` API is called: job queuing, batching, caching, retrying, worker pooling, and full observability, just like youâ€™d expect in a real world deployment powering millions of requests.

> ğŸ§  Built to demonstrate real-world AI Systems Engineering skills â€” infra, latency, load, and failure handling â€” **not just another model deployment.**

---

## âš¡ TL;DR

Inferyx is a containerized AI inference simulation system built for infra engineers.  
It features async queuing, batching, GPU-style worker pools, retry queues, Prometheus metrics, Grafana dashboards, and dynamic scaling â€” designed to showcase real-world system design skills under pressure.  
Built for the kind of AI engineers who know that getting a `/predict` call to survive production is a blood sport.

---

## ğŸš€ Features

- ğŸ” **Async Inference Queue** with Redis-based queuing
- ğŸ“¦ **Batching Engine** that groups jobs for optimal throughput
- ğŸ”¨ **Simulated GPU Worker Pool** (with dynamic scaling up and down)
- âŒ **Failure Simulation + Retry Queue** with exponential backoff
- âš ï¸ **Rate Limiting** by setting a maximum queue size to prevent abuse and overload.
- âš¡ **Redis Cache Layer** with hit/miss metrics
- ğŸ“ˆ **Prometheus + Grafana Monitoring** (latency, retries, throughput, worker state)
- ğŸ”¥ **Load Testing Utility** to simulate thousands of requests
- ğŸ§ª **Multi-model Routing** (based on `model_id`)
- ğŸ³ Fully **containerized** with Docker + Docker Compose

---

## ğŸ¥ Live Demo: Dynamic Worker Scaling

> ğŸ‘€ Watch Inferyx respond to real-time load:
> Incoming requests flood the queue â†’ Batches form â†’ Workers scale up dynamically â†’ Throughput spikes â†’ System auto-scales back to idle.

![dynamic-workers](workers.gif)
> ğŸ” Dynamic Worker Scaling in Action:  
> As the inference load increases, the worker pool automatically scales up to handle the demand â€” then scales down when idle, saving resources.

---

## System Architecture

### ğŸ™‹ Who Should Look at This?

- **MLEs** looking for real-world systems that go beyond toy deployments  
- **Backend Engineers** interested in infra-level AI serving  
- **Hiring Managers** looking for infra-first AI engineers who know their Redis from their retry queues  
- **Students** trying to learn how *actual AI infrastructure works* in prod

![system architecture](image.png)
> ğŸ§  Inferyx System Architecture: Shows the full inference flow â€” from `/infer` request to batching, worker pool execution, retry logic, and Redis-backed observability.


---
## ğŸ“Š Real-Time Observability
![grafana-screenshot](grafana.png)
> ğŸ“Š Real-Time Observability: Metrics include queue sizes, latency percentiles, retry dynamics, failures/sec, throughput, and cache effectiveness. Designed to simulate production stress.


---

## â“ Why This Matters

Most AI projects stop at â€œdeploy a model on FastAPI.â€  
Inferyx goes beyond â€” it asks: what happens **after** the first 1000 clients hammer your system?

- How do you batch without starving?
- How do you scale workers dynamically?
- How do you track failures, retries, and throughput in real time?

This is the difference between a Kaggle notebook and real-world ML systems engineering.

---

## ğŸ“¦ How to Run (Local Dev)

### âœ… Prerequisites
- Docker
- Docker Compose

### ğŸ› ï¸ One-Command Boot

```bash
docker-compose up --build
```

This will spin up:
- FastAPI App
- Batching Engine
- Worker Pool
- Retry Worker
- Redis
- Prometheus
- Grafana

---

> NOTE: The real model integrated is a Financial Aspect Based Sentiment Analysis model. For more training and model details, see [here.](https://github.com/LakshyaSingh354/FABSA/tree/main/fabsa-model)

> Kindly download the `onnx` model and put it in the root directory with name "onnx". Download the model from [here](https://www.kaggle.com/models/lakshyasingh354/fabsa).

## ğŸ§ª Test It

### ğŸ”„ Send Inference Request

```bash
curl -X POST http://localhost:8000/infer \
    -H "Content-Type: application/json" \
    -d '{"input": "your text here", "model_id": "mock"}'
```
> Or make automatic requests using [`inferyx.py`](https://github.com/LakshyaSingh354/Inferyx/blob/main/inferyx.py). Configure the variables according to the testing parameters.

### ğŸ“Š Monitor the System

- **Prometheus**: [http://localhost:9090](http://localhost:9090)  
- **Grafana**: [http://localhost:3000](http://localhost:3000)  
    - Login: `admin` / `admin`
    - Dashboard: [*Inferyx*](http://localhost:3000/d/inferyx/inferyx)

---

## ğŸ“ˆ Metrics Tracked

- ğŸ” `inference_queue_size`, `worker_queue_size`, `retry_queue_size`
- ğŸ“¦ `batch_size`, `batches_processed`
- ğŸ§  `worker_utilization`
- ğŸ¢ `latency_p50`, `latency_p95`, `latency_p99`
- ğŸ” `retry_attempts`, `failed_jobs`
- ğŸ’¥ `cache_hits`, `cache_misses`
- ğŸ”‚ `requests_per_second`

---

## âš ï¸ Failure Handling

- X% of jobs are simulated to fail (configurable)
- Failed jobs pushed to a **retry queue**
- Retry loop uses **exponential backoff**
- Max retry attempts configurable
- Observability baked in (retry delay, attempts)

---

## ğŸ”§ Configurable Constants

Set via [`config.py`](https://github.com/LakshyaSingh354/Inferyx/blob/main/config/config.py):

```python
# Batching
INFERENCE_QUEUE_KEY = "inference_queue"
WORKER_QUEUE_KEY = "worker_queue"
MAX_BATCH_SIZE = 4
MAX_WAIT_TIME = 1.0

# Queue
MAX_QUEUE_SIZE = 500

# Retry
RETRY_QUEUE_KEY = "retry_queue"
MAX_RETRIES = 3
RETRY_BACKOFF_SECONDS = 2
MAX_BACKOFF_SECONDS = 30

# Workers
MAX_WORKERS = 16
MIN_WORKERS = 1
JOBS_PER_WORKER = 10
CHECK_INTERVAL = 5

# Metrics
PROMETHEUS_MULTIPROC_DIR = "/tmp/prometheus_multiproc"
METRICS_PORT = 8080
```

---
## ğŸ“ Repo Structure
```.
â””â”€â”€ Inferyx/
    â”œâ”€â”€ api/
    â”‚   â”œâ”€â”€ auth.py
    â”‚   â”œâ”€â”€ main.py
    â”‚   â””â”€â”€ schema.py
    â”œâ”€â”€ batch/
    â”‚   â””â”€â”€ batching_engine.py
    â”œâ”€â”€ caching/
    â”‚   â””â”€â”€ cache_inference.py
    â”œâ”€â”€ config/
    â”‚   â””â”€â”€ config.py
    â”œâ”€â”€ job_queue/
    â”‚   â”œâ”€â”€ job_store.py
    â”‚   â”œâ”€â”€ producer.py
    â”‚   â”œâ”€â”€ redis_client.py
    â”‚   â”œâ”€â”€ schema.py
    â”‚   â””â”€â”€ utils.py
    â”œâ”€â”€ metrics/
    â”‚   â”œâ”€â”€ metrics.py
    â”‚   â””â”€â”€ serve.py
    â”œâ”€â”€ model/
    â”‚   â”œâ”€â”€ FABSA.py
    â”‚   â”œâ”€â”€ infer.py
    â”‚   â””â”€â”€ test_infer.py
    â”œâ”€â”€ retry/
    â”‚   â”œâ”€â”€ retry.py
    â”‚   â”œâ”€â”€ retry_worker.py
    â”‚   â””â”€â”€ utils.py
    â”œâ”€â”€ workers/
    â”‚   â”œâ”€â”€ worker_loop.py
    â”‚   â””â”€â”€ worker_pool.py
    â”œâ”€â”€ onnx/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ docker-compose.yaml
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ inferyx.py
    â”œâ”€â”€ prometheus.yaml
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ startup.sh
    â”œâ”€â”€ uv.lock
    â””â”€â”€ .python-version
```
---
## ğŸ§¾ TODO (Future Work)

- [ ] gRPC Gateway
- [ ] Model Registry Integration
- [ ] Kubernetes + HPA
- [ ] Triton Inference Server integration

---
## Contribute or Learn More

Open issues, ask questions, or suggest features.  
Letâ€™s build resilient AI infra together!  
â­ Star this repo if it helped you or inspired your own system designs.