# Inferyx - A Production-Grade AI Inference Engine

Inferyx is a high-performance inference system that simulates production-grade AI infrastructure.

It mimics what happens *after* a `/predict` API is called: job queuing, batching, caching, retrying, worker pooling, and full observability, just like youâ€™d expect in a real world deployment powering millions of requests.

> ğŸ§  Built to demonstrate real-world AI Systems Engineering skills â€” infra, latency, load, and failure handling â€” **not just another model deployment.**

---


## ğŸš€ Features

- ğŸ” **Async Inference Queue** with Redis-based queuing
- ğŸ“¦ **Batching Engine** that groups jobs for optimal throughput
- ğŸ”¨ **Simulated GPU Worker Pool** (with dynamic scaling up and down)
- âŒ **Failure Simulation + Retry Queue** with exponential backoff
- âš¡ **Redis Cache Layer** with hit/miss metrics
- ğŸ“ˆ **Prometheus + Grafana Monitoring** (latency, retries, throughput, worker state)
- ğŸ”¥ **Load Testing Utility** to simulate thousands of requests
- ğŸ§ª **Multi-model Routing** (based on `model_id`)
- ğŸ³ Fully **containerized** with Docker + Docker Compose

---

## System Architecture

![system architecture](image.png)

## ğŸ“¦ How to Run (Local Dev)

### âœ… Prerequisites
- Docker
- Docker Compose

### ğŸ› ï¸ One-Command Boot

```bash
docker-compose up --build
```

This will spin up:
- FastAPI App
- Batching Engine
- Worker Pool
- Retry Worker
- Redis
- Prometheus
- Grafana

---

> NOTE: The real model integrated is a Financial Aspect Based Sentiment Analysis model. For more training and model details, see [here.](https://github.com/LakshyaSingh354/FABSA/tree/main/fabsa-model)

> Kindly download the `onnx` model and put it in the root directory with name "onnx". Download the model from [here](https://www.kaggle.com/models/lakshyasingh354/fabsa).

## ğŸ§ª Test It

### ğŸ”„ Send Inference Request

```bash
curl -X POST http://localhost:8000/infer \
    -H "Content-Type: application/json" \
    -d '{"input": "your text here", "model_id": "mock"}'
```
> Or make automatic requests using `inferyx.py`. Configure the variables according to the testing parameters.

### ğŸ“Š Monitor the System

- **Prometheus**: [http://localhost:9090](http://localhost:9090)  
- **Grafana**: [http://localhost:3000](http://localhost:3000)  
    - Login: `admin` / `admin`
    - Dashboard: [*Inferyx*](http://localhost:3000/d/inferyx/inferyx)

---

## ğŸ“ˆ Metrics Tracked

- ğŸ” `inference_queue_size`, `worker_queue_size`, `retry_queue_size`
- ğŸ“¦ `batch_size`, `batches_processed`
- ğŸ§  `worker_utilization`
- ğŸ¢ `latency_p50`, `latency_p95`, `latency_p99`
- ğŸ” `retry_attempts`, `failed_jobs`
- ğŸ’¥ `cache_hits`, `cache_misses`
- ğŸ”‚ `requests_per_second`

---

## âš ï¸ Failure Handling

- X% of jobs are simulated to fail (configurable)
- Failed jobs pushed to a **retry queue**
- Retry loop uses **exponential backoff**
- Max retry attempts configurable
- Observability baked in (retry delay, attempts)

---

## ğŸ§  Design Philosophy

Inferyx is not a â€œdeploy a modelâ€ demo.  
Itâ€™s a **simulation of real AI infra problems**:

- What happens when 1000 clients hit `/infer`?
- Can jobs be batched without losing latency SLAs?
- How are failed jobs retried and monitored?
- Can you track the system under pressure?

---

## ğŸ“¸ Screenshots

![grafana-screenshot](grafana.png)

Dynamic Scaling up and down.
![dynamic-workers](workers.gif)
---

## ğŸ”§ Configurable Constants

Set via `config.py`:

```python
# Batching
INFERENCE_QUEUE_KEY = "inference_queue"
WORKER_QUEUE_KEY = "worker_queue"
MAX_BATCH_SIZE = 4
MAX_WAIT_TIME = 1.0

# Queue
MAX_QUEUE_SIZE = 500

# Retry
RETRY_QUEUE_KEY = "retry_queue"
MAX_RETRIES = 3
RETRY_BACKOFF_SECONDS = 2
MAX_BACKOFF_SECONDS = 30

# Workers
MAX_WORKERS = 16
MIN_WORKERS = 1
JOBS_PER_WORKER = 10
CHECK_INTERVAL = 5

# Metrics
PROMETHEUS_MULTIPROC_DIR = "/tmp/prometheus_multiproc"
METRICS_PORT = 8080
```

---
## ğŸ“ Repo Structure
```.
â””â”€â”€ Inferyx/
    â”œâ”€â”€ api/
    â”‚   â”œâ”€â”€ auth.py
    â”‚   â”œâ”€â”€ main.py
    â”‚   â””â”€â”€ schema.py
    â”œâ”€â”€ batch/
    â”‚   â””â”€â”€ batching_engine.py
    â”œâ”€â”€ caching/
    â”‚   â””â”€â”€ cache_inference.py
    â”œâ”€â”€ config/
    â”‚   â””â”€â”€ config.py
    â”œâ”€â”€ job_queue/
    â”‚   â”œâ”€â”€ job_store.py
    â”‚   â”œâ”€â”€ producer.py
    â”‚   â”œâ”€â”€ redis_client.py
    â”‚   â”œâ”€â”€ schema.py
    â”‚   â””â”€â”€ utils.py
    â”œâ”€â”€ metrics/
    â”‚   â”œâ”€â”€ metrics.py
    â”‚   â””â”€â”€ serve.py
    â”œâ”€â”€ model/
    â”‚   â”œâ”€â”€ FABSA.py
    â”‚   â”œâ”€â”€ infer.py
    â”‚   â””â”€â”€ test_infer.py
    â”œâ”€â”€ retry/
    â”‚   â”œâ”€â”€ retry.py
    â”‚   â”œâ”€â”€ retry_worker.py
    â”‚   â””â”€â”€ utils.py
    â”œâ”€â”€ workers/
    â”‚   â”œâ”€â”€ worker_loop.py
    â”‚   â””â”€â”€ worker_pool.py
    â”œâ”€â”€ onnx/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ docker-compose.yaml
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ inferyx.py
    â”œâ”€â”€ prometheus.yaml
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ startup.sh
    â”œâ”€â”€ uv.lock
    â””â”€â”€ .python-version
```
---
## ğŸ§¾ TODO (Future Work)

- [ ] gRPC Gateway
- [ ] Model Registry Integration
- [ ] Kubernetes + HPA
- [ ] Triton Inference Server integration